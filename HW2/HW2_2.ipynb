{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presteps to Load llama3.2 On Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 10:35:06.012553: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731580506.027052 3360610 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731580506.031268 3360610 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-14 10:35:06.045584: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Info: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Total RAM: 15.489391326904297 GB\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from psutil import virtual_memory\n",
    "\n",
    "# Check GPU\n",
    "gpu_info = tf.config.list_physical_devices('GPU')\n",
    "print(f\"GPU Info: {gpu_info}\")\n",
    "\n",
    "# Check RAM\n",
    "ram_info = virtual_memory()\n",
    "print(f\"Total RAM: {ram_info.total / (1024**3)} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt-get install -y pciutils\n",
    "# !curl -fsSL https://ollama.com/install.sh | sh # download ollama api\n",
    "# from IPython.display import clear_output\n",
    "\n",
    "# # Create a Python script to start the Ollama API server in a separate thread\n",
    "\n",
    "# import os\n",
    "# import threading\n",
    "# import subprocess\n",
    "# import requests\n",
    "# import json\n",
    "\n",
    "# def ollama():\n",
    "#     os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
    "#     os.environ['OLLAMA_ORIGINS'] = '*'\n",
    "#     subprocess.Popen([\"ollama\", \"serve\"])\n",
    "\n",
    "# ollama_thread = threading.Thread(target=ollama)\n",
    "# ollama_thread.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import clear_output\n",
    "# !ollama pull llama3.2:3b  & ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presteps to Load llama3.2 Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hardware Requirements** <br>\n",
    "**CPU**: Multicore processor<br>\n",
    "**RAM**: Minimum of 16 GB recommended<br>\n",
    "**GPU**: NVIDIA RTX series (for optimal performance), at least 8 GB VRAM<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step1**:<br>\n",
    "Download ollama from this site according to your operating system<br>\n",
    "https://ollama.com/download/linux<br>\n",
    "<br>\n",
    "**Step2**:<br>\n",
    "open your teminal<br>\n",
    "<br>\n",
    "**Step3**:<br>\n",
    "run following commands in your terminal<br>\n",
    "\\$ ollama serve<br>\n",
    "\\$ ollama pull llama3.2:3b  & ollama pull nomic-embed-text<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LlaMA3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tfpy7Ez7UN4v"
   },
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "# Initialize the Llama model\n",
    "model = OllamaLLM(model=MODEL)\n",
    "\n",
    "# Create an embedding model\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86LvINhAWLl-",
    "outputId": "b195417f-e5f9-4d2e-abd8-88c6eb16a9e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Yes, I'm an instance of Llama, a large language model developed by Meta. I'm designed to understand and respond to human language in a helpful and informative way. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "print(model.invoke(\"Hi. Are you LlaMA, the language model?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = os.getenv(\"LANGCHAIN_ENDPOINT\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zs0lRQIuDFw1"
   },
   "source": [
    "## Part1 Standard RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "w_93yT72DHsW",
    "outputId": "6d140c4b-2e8b-48d6-bab8-a1a6c99bf316"
   },
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_chroma.vectorstores import Chroma\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from chromadb.errors import InvalidDimensionException\n",
    "from langchain.chains import LLMChain\n",
    "#### INDEXING ####\n",
    "\n",
    "loader = PyPDFLoader(\"RAG_survey.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed\n",
    "## NOTE: you must run Chroma().delete_collection() before load the Chroma vectorstore \n",
    "## to delete previous loaded documents.\n",
    "Chroma().delete_collection()\n",
    "vectorstore = Chroma.from_documents(documents = splits, embedding=embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\n",
    "    \"rlm/rag-prompt\", \n",
    "    api_key=os.environ[\"LANGCHAIN_API_KEY\"],\n",
    "    api_url=os.environ[\"LANGCHAIN_ENDPOINT\"],\n",
    ")\n",
    "\n",
    "# LLM\n",
    "llm = model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Chain the Components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever|format_docs, \"question\": RunnablePassthrough()}|\n",
    "    ## TODO: complete the chain here\n",
    "    prompt | llm | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The paper appears to discuss the Research Activated Generation (RAG) paradigm for question-answering tasks, categorizing it into three stages and exploring its core components, optimization methods, post-retrieval process, and resistance to adversarial inputs. It also examines current challenges and limitations of RAG, including its potential shortcomings in handling specific types of documents. The paper aims to improve the performance of RAG by addressing these limitations.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"what is this paper about?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Explain TextSplitter Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Experiment with Retriever Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## TODO: Try some different settings for the retriever and output some examples\n",
    "# ## you can cahnge the question if you want\n",
    "# ## you can duplicate this cell to ouput different examples\n",
    "# retriever = vectorstore.as_retriever(...)\n",
    "# retrived_docs= retriever.invoke(\"what is this paper about?\")\n",
    "# for doc in retrived_docs:\n",
    "#     print()\n",
    "#     print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting: k=3\n",
      "page_content='question, form a comprehensive prompt that empowers LLMs\n",
      "to generate a well-informed answer.\n",
      "The RAG research paradigm is continuously evolving, and\n",
      "we categorize it into three stages: Naive RAG, Advanced\n",
      "RAG, and Modular RAG, as showed in Figure 3. Despite\n",
      "RAG method are cost-effective and surpass the performance\n",
      "of the native LLM, they also exhibit several limitations.\n",
      "The development of Advanced RAG and Modular RAG is\n",
      "a response to these specific shortcomings in Naive RAG.\n",
      "A. Naive RAG' metadata={'page': 1, 'source': 'RAG_survey.pdf'}\n",
      "\n",
      "page_content='irrelevant context,” in International Conference on Machine Learning .\n",
      "PMLR, 2023, pp. 31 210–31 227.\n",
      "[88] R. Teja, “Evaluating the ideal chunk size for a rag\n",
      "system using llamaindex,” https://www.llamaindex.ai/blog/\n",
      "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5,\n",
      "2023.' metadata={'page': 17, 'source': 'RAG_survey.pdf'}\n",
      "\n",
      "page_content='The paper unfolds as follows: Section II introduces the\n",
      "main concept and current paradigms of RAG. The following\n",
      "three sections explore core components—“Retrieval”, “Gen-\n",
      "eration” and “Augmentation”, respectively. Section III focuses\n",
      "on optimization methods in retrieval,including indexing, query\n",
      "and embedding optimization. Section IV concentrates on post-\n",
      "retrieval process and LLM fine-tuning in generation. Section V\n",
      "analyzes the three augmentation processes. Section VI focuses' metadata={'page': 1, 'source': 'RAG_survey.pdf'}\n",
      "\n",
      "Setting: search_type=similarity, k=3\n",
      "page_content='question, form a comprehensive prompt that empowers LLMs\n",
      "to generate a well-informed answer.\n",
      "The RAG research paradigm is continuously evolving, and\n",
      "we categorize it into three stages: Naive RAG, Advanced\n",
      "RAG, and Modular RAG, as showed in Figure 3. Despite\n",
      "RAG method are cost-effective and surpass the performance\n",
      "of the native LLM, they also exhibit several limitations.\n",
      "The development of Advanced RAG and Modular RAG is\n",
      "a response to these specific shortcomings in Naive RAG.\n",
      "A. Naive RAG' metadata={'page': 1, 'source': 'RAG_survey.pdf'}\n",
      "\n",
      "page_content='irrelevant context,” in International Conference on Machine Learning .\n",
      "PMLR, 2023, pp. 31 210–31 227.\n",
      "[88] R. Teja, “Evaluating the ideal chunk size for a rag\n",
      "system using llamaindex,” https://www.llamaindex.ai/blog/\n",
      "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5,\n",
      "2023.' metadata={'page': 17, 'source': 'RAG_survey.pdf'}\n",
      "\n",
      "page_content='The paper unfolds as follows: Section II introduces the\n",
      "main concept and current paradigms of RAG. The following\n",
      "three sections explore core components—“Retrieval”, “Gen-\n",
      "eration” and “Augmentation”, respectively. Section III focuses\n",
      "on optimization methods in retrieval,including indexing, query\n",
      "and embedding optimization. Section IV concentrates on post-\n",
      "retrieval process and LLM fine-tuning in generation. Section V\n",
      "analyzes the three augmentation processes. Section VI focuses' metadata={'page': 1, 'source': 'RAG_survey.pdf'}\n",
      "\n",
      "Setting: k=5\n",
      "page_content='question, form a comprehensive prompt that empowers LLMs\n",
      "to generate a well-informed answer.\n",
      "The RAG research paradigm is continuously evolving, and\n",
      "we categorize it into three stages: Naive RAG, Advanced\n",
      "RAG, and Modular RAG, as showed in Figure 3. Despite\n",
      "RAG method are cost-effective and surpass the performance\n",
      "of the native LLM, they also exhibit several limitations.\n",
      "The development of Advanced RAG and Modular RAG is\n",
      "a response to these specific shortcomings in Naive RAG.\n",
      "A. Naive RAG' metadata={'page': 1, 'source': 'RAG_survey.pdf'}\n",
      "\n",
      "page_content='irrelevant context,” in International Conference on Machine Learning .\n",
      "PMLR, 2023, pp. 31 210–31 227.\n",
      "[88] R. Teja, “Evaluating the ideal chunk size for a rag\n",
      "system using llamaindex,” https://www.llamaindex.ai/blog/\n",
      "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5,\n",
      "2023.' metadata={'page': 17, 'source': 'RAG_survey.pdf'}\n",
      "\n",
      "page_content='The paper unfolds as follows: Section II introduces the\n",
      "main concept and current paradigms of RAG. The following\n",
      "three sections explore core components—“Retrieval”, “Gen-\n",
      "eration” and “Augmentation”, respectively. Section III focuses\n",
      "on optimization methods in retrieval,including indexing, query\n",
      "and embedding optimization. Section IV concentrates on post-\n",
      "retrieval process and LLM fine-tuning in generation. Section V\n",
      "analyzes the three augmentation processes. Section VI focuses' metadata={'page': 1, 'source': 'RAG_survey.pdf'}\n",
      "\n",
      "page_content='situation is figuratively referred to as “Misinformation can\n",
      "be worse than no information at all”. Improving RAG’s\n",
      "resistance to such adversarial or counterfactual inputs is gain-\n",
      "ing research momentum and has become a key performance\n",
      "metric [48], [50], [82]. Cuconasu et al. [54] analyze which\n",
      "type of documents should be retrieved, evaluate the relevance\n",
      "of the documents to the prompt, their position, and the\n",
      "number included in the context. The research findings reveal' metadata={'page': 13, 'source': 'RAG_survey.pdf'}\n",
      "\n",
      "page_content='analyzes the three augmentation processes. Section VI focuses\n",
      "on RAG’s downstream tasks and evaluation system. Sec-\n",
      "tion VII mainly discusses the challenges that RAG currently\n",
      "faces and its future development directions. At last, the paper\n",
      "concludes in Section VIII.\n",
      "II. O VERVIEW OF RAG\n",
      "A typical application of RAG is illustrated in Figure 2.\n",
      "Here, a user poses a question to ChatGPT about a recent,\n",
      "widely discussed news. Given ChatGPT’s reliance on pre-' metadata={'page': 1, 'source': 'RAG_survey.pdf'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 例子1：设置返回的文档数量 k\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "retrieved_docs = retriever.invoke(\"what is this paper about?\")\n",
    "print(\"Setting: k=3\")\n",
    "for doc in retrieved_docs:\n",
    "    print(doc)\n",
    "    print()\n",
    "\n",
    "# 例子2：更改检索类型，例如向量搜索或相似度搜索\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "retrieved_docs = retriever.invoke(\"what is this paper about?\")\n",
    "print(\"Setting: search_type=similarity, k=3\")\n",
    "for doc in retrieved_docs:\n",
    "    print(doc)\n",
    "    print()\n",
    "\n",
    "# 例子3：尝试更大范围的文档数量\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "retrieved_docs = retriever.invoke(\"what is this paper about?\")\n",
    "print(\"Setting: k=5\")\n",
    "for doc in retrieved_docs:\n",
    "    print(doc)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part2 Multi-Query RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Prompt Template for Multi-Query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "## TODO: Please design a prompt template that instructs the language model to respond to questions from multiple perspectives.\n",
    "template = \"\"\"..... Original question: {question}\"\"\"\n",
    "\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may generate some queries here to see if the queries diverse enough\n",
    "question = \"What is this paper about?\"\n",
    "generate_queries.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Retrieve\n",
    "question = \"What is this paper about?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Multi-Query RAG Chain: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "## TODO: Coctruct a Multi-Query RAG Chain.\n",
    "# Hint1: use the retrieval_chain in this chain\n",
    "# Hint2: consider the format of the prompt above and also use it in the chain \n",
    "multi_query_rag_chain = (...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Example Comparisons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO:  show a standard RAG output example alongside a multi-query RAG output example.\n",
    "# Hint1: You may adjust the question to highlight the advantages of multi-query RAG over standard RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part3 RAG Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Use the same templat as Part2\n",
    "template = ...\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Implement Reciprocal Rank Fusion (RRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(results: list[list], c):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            ## TODO:  Implement Reciprocal Rank Fusion here\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) RRF Example and k-Value Discussion: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Provide an example showing the documents after re-ranking using RRF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) RAG Fusion Chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "## TODO: Implement the RAG Fusion chain \n",
    "# Hint1: use the retrieval_chain_rag_fusion in this chain\n",
    "# Hint2: consider the format of the prompt above and also use it in the chain \n",
    "rag_fusion_chain = (...)\n",
    "\n",
    "rag_fusion_chain.invoke({\"question\":question})"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
