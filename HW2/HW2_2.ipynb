{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Presteps to Load llama3.2 On Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from psutil import virtual_memory\n",
        "\n",
        "# Check GPU\n",
        "gpu_info = tf.config.list_physical_devices('GPU')\n",
        "print(f\"GPU Info: {gpu_info}\")\n",
        "\n",
        "# Check RAM\n",
        "ram_info = virtual_memory()\n",
        "print(f\"Total RAM: {ram_info.total / (1024**3)} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!sudo apt-get install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh # download ollama api\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Create a Python script to start the Ollama API server in a separate thread\n",
        "\n",
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def ollama():\n",
        "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "ollama_thread = threading.Thread(target=ollama)\n",
        "ollama_thread.start()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "!ollama pull llama3.2:3b  & ollama pull nomic-embed-text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Presteps to Load llama3.2 Locally"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Hardware Requirements** <br>\n",
        "**CPU**: Multicore processor<br>\n",
        "**RAM**: Minimum of 16 GB recommended<br>\n",
        "**GPU**: NVIDIA RTX series (for optimal performance), at least 8 GB VRAM<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step1**:<br>\n",
        "Download ollama from this site according to your operating system<br>\n",
        "https://ollama.com/download/linux<br>\n",
        "<br>\n",
        "**Step2**:<br>\n",
        "open your teminal<br>\n",
        "<br>\n",
        "**Step3**:<br>\n",
        "run following commands in your terminal<br>\n",
        "\\$ ollama serve<br>\n",
        "\\$ ollama pull llama3.2:3b  & ollama pull nomic-embed-text<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load LlaMA3.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "id": "tfpy7Ez7UN4v"
      },
      "outputs": [],
      "source": [
        "from langchain_community.llms import Ollama\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "\n",
        "MODEL = \"llama3.2:3b\"\n",
        "\n",
        "# Initialize the Llama model\n",
        "model = Ollama(model=MODEL)\n",
        "\n",
        "# Create an embedding model\n",
        "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86LvINhAWLl-",
        "outputId": "b195417f-e5f9-4d2e-abd8-88c6eb16a9e6"
      },
      "outputs": [],
      "source": [
        "print(model.invoke(\"Hi. Are you LlaMA, the language model?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs0lRQIuDFw1"
      },
      "source": [
        "## Part1 Standard RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "w_93yT72DHsW",
        "outputId": "6d140c4b-2e8b-48d6-bab8-a1a6c99bf316"
      },
      "outputs": [],
      "source": [
        "from langchain import hub\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from chromadb.errors import InvalidDimensionException\n",
        "\n",
        "#### INDEXING ####\n",
        "\n",
        "loader = PyPDFLoader(\"RAG_survey.pdf\")\n",
        "docs = loader.load()\n",
        "\n",
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# Embed\n",
        "## NOTE: you must run Chroma().delete_collection() before load the Chroma vectorstore \n",
        "## to delete previous loaded documents.\n",
        "Chroma().delete_collection()\n",
        "vectorstore = Chroma.from_documents(documents = splits, embedding=embeddings)\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# Prompt\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "# LLM\n",
        "llm = model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (a) Chain the Components:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Chain\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever|format_docs, \"question\": RunnablePassthrough()}|\n",
        "    ## TODO: complete the chain here\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rag_chain.invoke(\"what is this paper about?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (b) Explain TextSplitter Settings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Discussion:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (c) Experiment with Retriever Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TODO: Try some different settings for the retriever and output some examples\n",
        "## you can cahnge the question if you want\n",
        "## you can duplicate this cell to ouput different examples\n",
        "retriever = vectorstore.as_retriever(...)\n",
        "retrived_docs= retriever.invoke(\"what is this paper about?\")\n",
        "for doc in retrived_docs:\n",
        "    print()\n",
        "    print(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Discussion:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part2 Multi-Query RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (a) Prompt Template for Multi-Query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "## TODO: Please design a prompt template that instructs the language model to respond to questions from multiple perspectives.\n",
        "template = \"\"\"..... Original question: {question}\"\"\"\n",
        "\n",
        "prompt_perspectives = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [],
      "source": [
        "generate_queries = (\n",
        "    prompt_perspectives \n",
        "    | llm\n",
        "    | StrOutputParser() \n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# You may generate some queries here to see if the queries diverse enough\n",
        "question = \"What is this paper about?\"\n",
        "generate_queries.invoke({\"question\": question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.load import dumps, loads\n",
        "\n",
        "def get_unique_union(documents: list[list]):\n",
        "    \"\"\" Unique union of retrieved docs \"\"\"\n",
        "    # Flatten list of lists, and convert each Document to string\n",
        "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
        "    # Get unique documents\n",
        "    unique_docs = list(set(flattened_docs))\n",
        "    # Return\n",
        "    return [loads(doc) for doc in unique_docs]\n",
        "\n",
        "# Retrieve\n",
        "question = \"What is this paper about?\"\n",
        "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
        "docs = retrieval_chain.invoke({\"question\":question})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (b) Multi-Query RAG Chain: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "# RAG\n",
        "template = \"\"\"Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "\n",
        "## TODO: Coctruct a Multi-Query RAG Chain.\n",
        "# Hint1: use the retrieval_chain in this chain\n",
        "# Hint2: consider the format of the prompt above and also use it in the chain \n",
        "multi_query_rag_chain = (...)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (c) Example Comparisons:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TODO:  show a standard RAG output example alongside a multi-query RAG output example.\n",
        "# Hint1: You may adjust the question to highlight the advantages of multi-query RAG over standard RAG."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Discussion:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part3 RAG Fusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TODO: Use the same templat as Part2\n",
        "template = ...\n",
        "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {},
      "outputs": [],
      "source": [
        "generate_queries = (\n",
        "    prompt_rag_fusion \n",
        "    | llm\n",
        "    | StrOutputParser() \n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generate_queries.invoke(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (a) Implement Reciprocal Rank Fusion (RRF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reciprocal_rank_fusion(results: list[list], c):\n",
        "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
        "        and an optional parameter k used in the RRF formula \"\"\"\n",
        "    \n",
        "    # Initialize a dictionary to hold fused scores for each unique document\n",
        "    fused_scores = {}\n",
        "\n",
        "    # Iterate through each list of ranked documents\n",
        "    for docs in results:\n",
        "        # Iterate through each document in the list, with its rank (position in the list)\n",
        "        for rank, doc in enumerate(docs):\n",
        "            ## TODO:  Implement Reciprocal Rank Fusion here\n",
        "\n",
        "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
        "    reranked_results = [\n",
        "        (loads(doc), score)\n",
        "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    ]\n",
        "\n",
        "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
        "    return reranked_results\n",
        "\n",
        "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (b) RRF Example and k-Value Discussion: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TODO: Provide an example showing the documents after re-ranking using RRF."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Discussion:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (c) RAG Fusion Chain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RAG\n",
        "template = \"\"\"Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "\n",
        "## TODO: Implement the RAG Fusion chain \n",
        "# Hint1: use the retrieval_chain_rag_fusion in this chain\n",
        "# Hint2: consider the format of the prompt above and also use it in the chain \n",
        "rag_fusion_chain = (...)\n",
        "\n",
        "rag_fusion_chain.invoke({\"question\":question})"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
