{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presteps to Load llama3.2 On Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Info: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Total RAM: 15.489391326904297 GB\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from psutil import virtual_memory\n",
    "\n",
    "# Check GPU\n",
    "gpu_info = tf.config.list_physical_devices('GPU')\n",
    "print(f\"GPU Info: {gpu_info}\")\n",
    "\n",
    "# Check RAM\n",
    "ram_info = virtual_memory()\n",
    "print(f\"Total RAM: {ram_info.total / (1024**3)} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt-get install -y pciutils\n",
    "# !curl -fsSL https://ollama.com/install.sh | sh # download ollama api\n",
    "# from IPython.display import clear_output\n",
    "\n",
    "# # Create a Python script to start the Ollama API server in a separate thread\n",
    "\n",
    "# import os\n",
    "# import threading\n",
    "# import subprocess\n",
    "# import requests\n",
    "# import json\n",
    "\n",
    "# def ollama():\n",
    "#     os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
    "#     os.environ['OLLAMA_ORIGINS'] = '*'\n",
    "#     subprocess.Popen([\"ollama\", \"serve\"])\n",
    "\n",
    "# ollama_thread = threading.Thread(target=ollama)\n",
    "# ollama_thread.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import clear_output\n",
    "# !ollama pull llama3.2:3b  & ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presteps to Load llama3.2 Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hardware Requirements** <br>\n",
    "**CPU**: Multicore processor<br>\n",
    "**RAM**: Minimum of 16 GB recommended<br>\n",
    "**GPU**: NVIDIA RTX series (for optimal performance), at least 8 GB VRAM<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step1**:<br>\n",
    "Download ollama from this site according to your operating system<br>\n",
    "https://ollama.com/download/linux<br>\n",
    "<br>\n",
    "**Step2**:<br>\n",
    "open your teminal<br>\n",
    "<br>\n",
    "**Step3**:<br>\n",
    "run following commands in your terminal<br>\n",
    "\\$ ollama serve<br>\n",
    "\\$ ollama pull llama3.2:3b  & ollama pull nomic-embed-text<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LlaMA3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tfpy7Ez7UN4v"
   },
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "# Initialize the Llama model\n",
    "model = OllamaLLM(model=MODEL)\n",
    "\n",
    "# Create an embedding model\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86LvINhAWLl-",
    "outputId": "b195417f-e5f9-4d2e-abd8-88c6eb16a9e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I am indeed a variant of the Llama model, specifically designed to assist and communicate with users like you. I'm here to provide information, answer questions, and engage in conversations to the best of my abilities based on my training data. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "print(model.invoke(\"Hi. Are you LlaMA, the language model?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = os.getenv(\"LANGCHAIN_ENDPOINT\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zs0lRQIuDFw1"
   },
   "source": [
    "## Part1 Standard RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "w_93yT72DHsW",
    "outputId": "6d140c4b-2e8b-48d6-bab8-a1a6c99bf316"
   },
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_chroma.vectorstores import Chroma\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from chromadb.errors import InvalidDimensionException\n",
    "from langchain.chains import LLMChain\n",
    "#### INDEXING ####\n",
    "\n",
    "loader = PyPDFLoader(\"RAG_survey.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed\n",
    "## NOTE: you must run Chroma().delete_collection() before load the Chroma vectorstore \n",
    "## to delete previous loaded documents.\n",
    "Chroma().delete_collection()\n",
    "vectorstore = Chroma.from_documents(documents = splits, embedding=embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\n",
    "    \"rlm/rag-prompt\", \n",
    "    api_key=os.environ[\"LANGCHAIN_API_KEY\"],\n",
    "    api_url=os.environ[\"LANGCHAIN_ENDPOINT\"],\n",
    ")\n",
    "\n",
    "# LLM\n",
    "llm = model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Chain the Components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever|format_docs, \"question\": RunnablePassthrough()}|\n",
    "    ## TODO: complete the chain here\n",
    "    prompt | llm | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This paper appears to be about Research on Active Retrieval Architectures (RAG), discussing its evolution into three stages: Naive RAG, Advanced RAG, and Modular RAG, with a focus on addressing limitations and optimizing retrieval methods. It explores core components of RAG, including retrieval, generation, and augmentation, as well as post-retrieval process optimization. The paper aims to improve RAG's resistance to adversarial inputs and evaluate its performance metrics.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"what is this paper about?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Explain TextSplitter Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用 chunk_size=500 和 chunk_overlap=100 的結果：\n",
      "This paper is about the Research Agenda on Generative Models (RAG), specifically its evolution into three stages (Naive RAG, Advanced RAG, and Modular RAG) and the limitations of the RAG method compared to native Large Language Models (LLMs). The paper explores various components and optimization methods in RAG, as well as the importance of improving its resistance to adversarial inputs. It also discusses recent research on augmentation processes and retrieval methods for RAG.\n",
      "使用 chunk_size=1000 和 chunk_overlap=200 的結果：\n",
      "I don't know, but based on the context provided, it appears that this paper is about a research paradigm called RAG (Research Artifact Generation), which is a method for generating answers to questions using large language models. The paper discusses the development of Advanced and Modular RAG as responses to limitations in Naive RAG, with an emphasis on its adaptability, efficiency, and evaluation abilities.\n",
      "使用 chunk_size=200 和 chunk_overlap=50 的結果：\n",
      "I don't know. However, based on the provided context, it appears that the paper discusses \"Eli5\" and its future directions for the Role-Labeling Algorithm Generator (RAG). The paper provides an overview of RAG's applications and potential enhancements.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_chroma.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from chromadb.errors import InvalidDimensionException\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# 加載 PDF 文件\n",
    "loader = PyPDFLoader(\"RAG_survey.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# 定義不同的 chunk_size 和 chunk_overlap 設置\n",
    "settings = [\n",
    "    {\"chunk_size\": 500, \"chunk_overlap\": 100},\n",
    "    {\"chunk_size\": 1000, \"chunk_overlap\": 200},\n",
    "    {\"chunk_size\": 200, \"chunk_overlap\": 50},\n",
    "]\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 初始化嵌入模型\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "# 初始化 LLM 模型\n",
    "llm = OllamaLLM(model=\"llama3.2:3b\")\n",
    "\n",
    "# 提示模板\n",
    "prompt = hub.pull(\n",
    "    \"rlm/rag-prompt\", \n",
    "    api_key=os.environ[\"LANGCHAIN_API_KEY\"],\n",
    "    api_url=os.environ[\"LANGCHAIN_ENDPOINT\"],\n",
    ")\n",
    "\n",
    "question = \"what is this paper about?\"\n",
    "\n",
    "# 循環遍歷不同的設置\n",
    "for setting in settings:\n",
    "    print(f\"使用 chunk_size={setting['chunk_size']} 和 chunk_overlap={setting['chunk_overlap']} 的結果：\")\n",
    "\n",
    "    # 分割文本\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=setting['chunk_size'], chunk_overlap=setting['chunk_overlap'])\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    # 嵌入文本\n",
    "    Chroma().delete_collection()\n",
    "    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    # 構建檢索鏈\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever|format_docs, \"question\": RunnablePassthrough()}|\n",
    "        ## TODO: complete the chain here\n",
    "        prompt | llm | output_parser\n",
    "    )\n",
    "\n",
    "    # 執行檢索鏈\n",
    "    res = rag_chain.invoke(question)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "\n",
    "在文本檢索過程中，TextSplitter 的設置對於結果的準確性和效率有著重要的影響。TextSplitter 的主要參數包括塊大小（chunk size）、塊重疊（chunk overlap）以及其他相關參數。\n",
    "\n",
    "首先，塊大小（chunk size）決定了每個文本塊的長度。較大的塊大小可以包含更多的上下文信息，有助於提高檢索結果的準確性。然而，過大的塊大小可能會導致處理時間增加，並且在某些情況下，可能會包含過多無關的信息。相反，較小的塊大小可以提高檢索的速度，但可能會因為上下文信息不足而降低準確性。因此，選擇合適的塊大小需要在準確性和效率之間取得平衡。\n",
    "\n",
    "其次，塊重疊（chunk overlap）是指相鄰文本塊之間的重疊部分。適當的重疊可以確保重要的信息不會因為被分割到不同的塊中而丟失，從而提高檢索結果的完整性和準確性。過大的重疊會增加處理的冗餘度，從而影響效率；而過小的重疊則可能導致信息丟失。因此，合理設置塊重疊是確保檢索質量的重要因素。\n",
    "\n",
    "除了塊大小和塊重疊，其他參數如分割策略（例如按句子、段落或固定長度分割）也會影響檢索過程。按句子或段落分割可以保留自然語言的結構，從而提高檢索結果的可讀性和相關性；而固定長度分割則可能更適合於結構化數據的處理。\n",
    "\n",
    "根據不同的 `chunk_size` 和 `chunk_overlap` 設置，檢索結果顯示了明顯的差異，這些差異反映了文本分割策略對檢索過程的影響。\n",
    "\n",
    "使用 `chunk_size=500` 和 `chunk_overlap=100` 的結果顯示，這種設置能夠提供較為詳細和準確的回答。這是因為適中的塊大小和適當的重疊確保了足夠的上下文信息，同時避免了過多的冗餘。這樣的設置在保留文本完整性的同時，也能夠有效地處理和檢索信息。\n",
    "\n",
    "使用 `chunk_size=1000` 和 `chunk_overlap=200` 的結果顯示，這種設置可能導致過多的上下文信息被包含在單個文本塊中，從而影響了檢索的準確性。過大的塊大小可能會使模型難以聚焦於關鍵信息，導致回答變得模糊和不確定。此外，過大的重疊也增加了處理的冗餘度，進一步影響了檢索效率。\n",
    "\n",
    "使用 `chunk_size=200` 和 `chunk_overlap=50` 的結果顯示，這種設置可能導致上下文信息不足，從而影響了檢索結果的完整性和準確性。過小的塊大小可能會切斷重要的上下文信息，使得模型無法充分理解文本內容，導致回答不夠準確和具體。\n",
    "\n",
    "總結來說，TextSplitter 的設置需要根據具體應用場景進行調整， `chunk_size` 和 `chunk_overlap` 的設置對檢索過程有著重要的影響。適中的塊大小和適當的重疊可以在保留上下文信息和避免冗餘之間取得平衡，從而提高檢索結果的準確性和效率。根據具體的應用場景和需求，合理調整這些參數可以顯著提升文本檢索系統的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Experiment with Retriever Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting: k=3\n",
      "page_content='question, form a comprehensive prompt that empowers LLMs\n",
      "to generate a well-informed answer.\n",
      "The RAG research paradigm is continuously evolving, and\n",
      "we categorize it into three stages: Naive RAG, Advanced\n",
      "RAG, and Modular RAG, as showed in Figure 3. Despite\n",
      "RAG method are cost-effective and surpass the performance\n",
      "of the native LLM, they also exhibit several limitations.\n",
      "The development of Advanced RAG and Modular RAG is\n",
      "a response to these specific shortcomings in Naive RAG.\n",
      "A. Naive RAG' metadata={'page': 1, 'source': 'RAG_survey.pdf'}\n",
      "\n",
      "page_content='irrelevant context,” in International Conference on Machine Learning .\n",
      "PMLR, 2023, pp. 31 210–31 227.\n",
      "[88] R. Teja, “Evaluating the ideal chunk size for a rag\n",
      "system using llamaindex,” https://www.llamaindex.ai/blog/\n",
      "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5,\n",
      "2023.' metadata={'page': 17, 'source': 'RAG_survey.pdf'}\n",
      "\n",
      "page_content='The paper unfolds as follows: Section II introduces the\n",
      "main concept and current paradigms of RAG. The following\n",
      "three sections explore core components—“Retrieval”, “Gen-\n",
      "eration” and “Augmentation”, respectively. Section III focuses\n",
      "on optimization methods in retrieval,including indexing, query\n",
      "and embedding optimization. Section IV concentrates on post-\n",
      "retrieval process and LLM fine-tuning in generation. Section V\n",
      "analyzes the three augmentation processes. Section VI focuses' metadata={'page': 1, 'source': 'RAG_survey.pdf'}\n",
      "\n",
      "Setting: search_type=similarity, k=3\n",
      "page_content='question, form a comprehensive prompt that empowers LLMs\n",
      "to generate a well-informed answer.\n",
      "The RAG research paradigm is continuously evolving, and\n",
      "we categorize it into three stages: Naive RAG, Advanced\n",
      "RAG, and Modular RAG, as showed in Figure 3. Despite\n",
      "RAG method are cost-effective and surpass the performance\n",
      "of the native LLM, they also exhibit several limitations.\n",
      "The development of Advanced RAG and Modular RAG is\n",
      "a response to these specific shortcomings in Naive RAG.\n",
      "A. Naive RAG' metadata={'page': 1, 'source': 'RAG_survey.pdf'}\n",
      "\n",
      "page_content='irrelevant context,” in International Conference on Machine Learning .\n",
      "PMLR, 2023, pp. 31 210–31 227.\n",
      "[88] R. Teja, “Evaluating the ideal chunk size for a rag\n",
      "system using llamaindex,” https://www.llamaindex.ai/blog/\n",
      "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5,\n",
      "2023.' metadata={'page': 17, 'source': 'RAG_survey.pdf'}\n",
      "\n",
      "page_content='The paper unfolds as follows: Section II introduces the\n",
      "main concept and current paradigms of RAG. The following\n",
      "three sections explore core components—“Retrieval”, “Gen-\n",
      "eration” and “Augmentation”, respectively. Section III focuses\n",
      "on optimization methods in retrieval,including indexing, query\n",
      "and embedding optimization. Section IV concentrates on post-\n",
      "retrieval process and LLM fine-tuning in generation. Section V\n",
      "analyzes the three augmentation processes. Section VI focuses' metadata={'page': 1, 'source': 'RAG_survey.pdf'}\n",
      "\n",
      "Setting: k=5\n",
      "page_content='question, form a comprehensive prompt that empowers LLMs\n",
      "to generate a well-informed answer.\n",
      "The RAG research paradigm is continuously evolving, and\n",
      "we categorize it into three stages: Naive RAG, Advanced\n",
      "RAG, and Modular RAG, as showed in Figure 3. Despite\n",
      "RAG method are cost-effective and surpass the performance\n",
      "of the native LLM, they also exhibit several limitations.\n",
      "The development of Advanced RAG and Modular RAG is\n",
      "a response to these specific shortcomings in Naive RAG.\n",
      "A. Naive RAG' metadata={'page': 1, 'source': 'RAG_survey.pdf'}\n",
      "\n",
      "page_content='irrelevant context,” in International Conference on Machine Learning .\n",
      "PMLR, 2023, pp. 31 210–31 227.\n",
      "[88] R. Teja, “Evaluating the ideal chunk size for a rag\n",
      "system using llamaindex,” https://www.llamaindex.ai/blog/\n",
      "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5,\n",
      "2023.' metadata={'page': 17, 'source': 'RAG_survey.pdf'}\n",
      "\n",
      "page_content='The paper unfolds as follows: Section II introduces the\n",
      "main concept and current paradigms of RAG. The following\n",
      "three sections explore core components—“Retrieval”, “Gen-\n",
      "eration” and “Augmentation”, respectively. Section III focuses\n",
      "on optimization methods in retrieval,including indexing, query\n",
      "and embedding optimization. Section IV concentrates on post-\n",
      "retrieval process and LLM fine-tuning in generation. Section V\n",
      "analyzes the three augmentation processes. Section VI focuses' metadata={'page': 1, 'source': 'RAG_survey.pdf'}\n",
      "\n",
      "page_content='situation is figuratively referred to as “Misinformation can\n",
      "be worse than no information at all”. Improving RAG’s\n",
      "resistance to such adversarial or counterfactual inputs is gain-\n",
      "ing research momentum and has become a key performance\n",
      "metric [48], [50], [82]. Cuconasu et al. [54] analyze which\n",
      "type of documents should be retrieved, evaluate the relevance\n",
      "of the documents to the prompt, their position, and the\n",
      "number included in the context. The research findings reveal' metadata={'page': 13, 'source': 'RAG_survey.pdf'}\n",
      "\n",
      "page_content='analyzes the three augmentation processes. Section VI focuses\n",
      "on RAG’s downstream tasks and evaluation system. Sec-\n",
      "tion VII mainly discusses the challenges that RAG currently\n",
      "faces and its future development directions. At last, the paper\n",
      "concludes in Section VIII.\n",
      "II. O VERVIEW OF RAG\n",
      "A typical application of RAG is illustrated in Figure 2.\n",
      "Here, a user poses a question to ChatGPT about a recent,\n",
      "widely discussed news. Given ChatGPT’s reliance on pre-' metadata={'page': 1, 'source': 'RAG_survey.pdf'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 例子1：设置返回的文档数量 k\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "retrieved_docs = retriever.invoke(\"what is this paper about?\")\n",
    "print(\"Setting: k=3\")\n",
    "for doc in retrieved_docs:\n",
    "    print(doc)\n",
    "    print()\n",
    "\n",
    "# 例子2：更改检索类型，例如向量搜索或相似度搜索\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "retrieved_docs = retriever.invoke(\"what is this paper about?\")\n",
    "print(\"Setting: search_type=similarity, k=3\")\n",
    "for doc in retrieved_docs:\n",
    "    print(doc)\n",
    "    print()\n",
    "\n",
    "# 例子3：尝试更大范围的文档数量\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "retrieved_docs = retriever.invoke(\"what is this paper about?\")\n",
    "print(\"Setting: k=5\")\n",
    "for doc in retrieved_docs:\n",
    "    print(doc)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "在這次實驗中，我們嘗試了不同的檢索器設置，包括 `k` 值（返回的頂部文檔數量）和檢索類型（如相似度檢索），並觀察了每種設置下檢索到的文檔內容。以下是對這些設置的討論及其適用場景。\n",
    "\n",
    "### 設置：k=3\n",
    "在這種設置下，檢索器返回了三個頂部文檔。這些文檔涵蓋了 RAG 研究範式的演變、主要組件和優化方法等關鍵內容。這種設置適合於需要快速獲取關鍵信息的場景，因為返回的文檔數量較少，可以更快地處理和分析。然而，這也意味著可能會遺漏一些有價值的信息，特別是在文檔數量較多且信息分散的情況下。\n",
    "\n",
    "### 設置：search_type=similarity, k=3\n",
    "在這種設置下，檢索器根據相似度返回了三個頂部文檔。結果顯示，這種設置能夠有效地找到與查詢最相關的文檔，並且返回的內容與 `k=3` 的設置相似。這表明相似度檢索在確保相關性方面具有良好的性能。這種設置適合於需要高相關性檢索結果的場景，例如精確回答具體問題或查找特定主題的信息。\n",
    "\n",
    "### 設置：k=5\n",
    "在這種設置下，檢索器返回了五個頂部文檔。相比於 `k=3` 的設置，這種設置能夠提供更多的信息，涵蓋了更廣泛的內容，包括 RAG 的挑戰和未來發展方向等。這種設置適合於需要全面了解某個主題的場景，因為返回的文檔數量更多，可以提供更豐富的上下文信息。然而，這也意味著需要更多的時間來處理和分析這些文檔。\n",
    "\n",
    "### 討論\n",
    "不同的檢索器設置在不同的場景下具有不同的適用性。較小的 `k` 值（如 `k=3`）適合於需要快速獲取關鍵信息的場景，而較大的 `k` 值（如 `k=5`）則適合於需要全面了解某個主題的場景。相似度檢索能夠確保返回的文檔與查詢高度相關，適合於需要高相關性檢索結果的場景。\n",
    "\n",
    "總結來說，選擇合適的檢索器設置需要根據具體的應用需求進行調整。在實際應用中，可以根據查詢的具體要求和場景，靈活調整 `k` 值和檢索類型，以獲得最佳的檢索效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part2 Multi-Query RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Prompt Template for Multi-Query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Design a prompt template that instructs the language model to respond to questions from multiple perspectives.\n",
    "template = \"\"\"\n",
    "You are an expert in various fields. Please provide answers to the following question from different perspectives:\n",
    "1. As a scientist\n",
    "2. As a historian\n",
    "3. As a philosopher\n",
    "4. As an economist\n",
    "\n",
    "Original question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    "    | (lambda x : list(filter(lambda x : x != '', x)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'll respond from four different perspectives:\",\n",
       " '**As a Scientist**',\n",
       " 'This paper appears to be a study on the effects of environmental pollution on human health and ecosystems. Based on the title, \"Impact of Human Activity on Ecological Balance,\" I would expect the paper to discuss the scientific methods used to collect data on pollutants in various environments, such as air quality, water contamination, and soil degradation. The researchers may also explore the biological consequences of these pollutants on human populations and the natural world.',\n",
       " '**As a Historian**',\n",
       " 'From a historical perspective, this paper seems to be an analysis of the relationship between human activity and ecological balance throughout time. I would expect the authors to have conducted extensive research on past events, such as industrial revolutions, colonialism, and environmental disasters, to understand how these activities impacted the natural world. The paper may also touch on the development of modern environmental policies and the role of science in informing these efforts.',\n",
       " '**As a Philosopher**',\n",
       " 'This paper seems to be an examination of the human condition and our relationship with the natural world. I would expect the authors to have explored philosophical concepts, such as the concept of \"the Other\" (i.e., non-human entities), and how they intersect with our understanding of ecological balance. The paper may also grapple with questions of moral responsibility, agency, and the human condition in light of environmental degradation.',\n",
       " '**As an Economist**',\n",
       " 'From an economic perspective, this paper appears to be a study on the costs and benefits of environmental policies aimed at mitigating the negative impacts of human activity on ecosystems. I would expect the authors to have analyzed data on the economic efficiency of various environmental interventions, such as carbon pricing or green infrastructure investments, and compared them to other policy options. The paper may also discuss the distributional effects of these policies and how they impact different groups within society.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You may generate some queries here to see if the queries diverse enough\n",
    "question = \"What is this paper about?\"\n",
    "generate_queries.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Retrieve\n",
    "question = \"What is this paper about?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 15, 'source': 'RAG_survey.pdf'}, page_content='VIII. C ONCLUSION\\nThe summary of this paper, as depicted in Figure 6, empha-\\nsizes RAG’s significant advancement in enhancing the capa-\\nbilities of LLMs by integrating parameterized knowledge from\\nlanguage models with extensive non-parameterized data from\\nexternal knowledge bases. The survey showcases the evolution\\nof RAG technologies and their application on many different\\ntasks. The analysis outlines three developmental paradigms\\nwithin the RAG framework: Naive, Advanced, and Modu-'),\n",
       " Document(metadata={'page': 16, 'source': 'RAG_survey.pdf'}, page_content='search,” arXiv preprint arXiv:2311.03758 , 2023.\\n[10] H. S. Zheng, S. Mishra, X. Chen, H.-T. Cheng, E. H. Chi, Q. V . Le,\\nand D. Zhou, “Take a step back: Evoking reasoning via abstraction in\\nlarge language models,” arXiv preprint arXiv:2310.06117 , 2023.\\n[11] L. Gao, X. Ma, J. Lin, and J. Callan, “Precise zero-shot dense retrieval\\nwithout relevance labels,” arXiv preprint arXiv:2212.10496 , 2022.\\n[12] V . Blagojevi, “Enhancing rag pipelines in haystack: Introducing diver-'),\n",
       " Document(metadata={'page': 1, 'source': 'RAG_survey.pdf'}, page_content='question, form a comprehensive prompt that empowers LLMs\\nto generate a well-informed answer.\\nThe RAG research paradigm is continuously evolving, and\\nwe categorize it into three stages: Naive RAG, Advanced\\nRAG, and Modular RAG, as showed in Figure 3. Despite\\nRAG method are cost-effective and surpass the performance\\nof the native LLM, they also exhibit several limitations.\\nThe development of Advanced RAG and Modular RAG is\\na response to these specific shortcomings in Naive RAG.\\nA. Naive RAG'),\n",
       " Document(metadata={'page': 13, 'source': 'RAG_survey.pdf'}, page_content='opportunities for the development of RAG, enabling it to\\naddress more complex problems and integrative or summary\\nquestions that require reading a large amount of material to\\nanswer [49]. Developing new RAG methods in the context of\\nsuper-long contexts is one of the future research trends.\\nB. RAG Robustness\\nThe presence of noise or contradictory information during\\nretrieval can detrimentally affect RAG’s output quality. This\\nsituation is figuratively referred to as “Misinformation can'),\n",
       " Document(metadata={'page': 18, 'source': 'RAG_survey.pdf'}, page_content='A question answering challenge targeting commonsense knowledge,”\\narXiv preprint arXiv:1811.00937 , 2018.\\n[133] E. Dinan, S. Roller, K. Shuster, A. Fan, M. Auli, and J. Weston,\\n“Wizard of wikipedia: Knowledge-powered conversational agents,”\\narXiv preprint arXiv:1811.01241 , 2018.\\n[134] H. Wang, M. Hu, Y . Deng, R. Wang, F. Mi, W. Wang, Y . Wang, W.-\\nC. Kwan, I. King, and K.-F. Wong, “Large language models as source'),\n",
       " Document(metadata={'page': 16, 'source': 'RAG_survey.pdf'}, page_content='arXiv:2308.11761, 2023.\\n[16] A. H. Raudaschl, “Forget rag, the future\\nis rag-fusion,” https://towardsdatascience.com/\\nforget-rag-the-future-is-rag-fusion-1147298d8ad1, 2023.\\n[17] X. Cheng, D. Luo, X. Chen, L. Liu, D. Zhao, and R. Yan, “Lift\\nyourself up: Retrieval-augmented text generation with self memory,”\\narXiv preprint arXiv:2305.02437 , 2023.\\n[18] S. Wang, Y . Xu, Y . Fang, Y . Liu, S. Sun, R. Xu, C. Zhu, and\\nM. Zeng, “Training data is more valuable than you think: A simple'),\n",
       " Document(metadata={'page': 18, 'source': 'RAG_survey.pdf'}, page_content='conference on empirical methods in natural language processing, 2013,\\npp. 1533–1544.\\n[116] A. Mallen, A. Asai, V . Zhong, R. Das, H. Hajishirzi, and D. Khashabi,\\n“When not to trust language models: Investigating effectiveness and\\nlimitations of parametric and non-parametric memories,” arXiv preprint\\narXiv:2212.10511, 2022.\\n[117] T. Nguyen, M. Rosenberg, X. Song, J. Gao, S. Tiwary, R. Majumder,\\nand L. Deng, “Ms marco: A human-generated machine reading com-\\nprehension dataset,” 2016.'),\n",
       " Document(metadata={'page': 15, 'source': 'RAG_survey.pdf'}, page_content='16\\nFig. 6. Summary of RAG ecosystem\\ninitial learning curve. 3) Specialization - optimizing RAG to\\nbetter serve production environments.\\nThe mutual growth of RAG models and their technology\\nstacks is evident; technological advancements continuously\\nestablish new standards for existing infrastructure. In turn,\\nenhancements to the technology stack drive the development\\nof RAG capabilities. RAG toolkits are converging into a\\nfoundational technology stack, laying the groundwork for'),\n",
       " Document(metadata={'page': 19, 'source': 'RAG_survey.pdf'}, page_content='E. Bakhturina, M. Shoeybi, and B. Catanzaro, “Retrieval meets long\\ncontext large language models,” arXiv preprint arXiv:2310.03025 ,\\n2023.\\n[171] C. Packer, V . Fang, S. G. Patil, K. Lin, S. Wooders, and J. E. Gon-\\nzalez, “Memgpt: Towards llms as operating systems,” arXiv preprint\\narXiv:2310.08560, 2023.\\n[172] G. Xiao, Y . Tian, B. Chen, S. Han, and M. Lewis, “Efficient\\nstreaming language models with attention sinks,” arXiv preprint\\narXiv:2309.17453, 2023.'),\n",
       " Document(metadata={'page': 16, 'source': 'RAG_survey.pdf'}, page_content='contributions to the AI research and development community.\\nREFERENCES\\n[1] N. Kandpal, H. Deng, A. Roberts, E. Wallace, and C. Raffel, “Large\\nlanguage models struggle to learn long-tail knowledge,” in Interna-\\ntional Conference on Machine Learning . PMLR, 2023, pp. 15 696–\\n15 707.\\n[2] Y . Zhang, Y . Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao,\\nY . Zhang, Y . Chenet al., “Siren’s song in the ai ocean: A survey on hal-'),\n",
       " Document(metadata={'page': 11, 'source': 'RAG_survey.pdf'}, page_content='noise robustness, negative rejection, information integration,\\nand counterfactual robustness [167], [168]. These abilities are\\ncritical for the model’s performance under various challenges\\nand complex scenarios, impacting the quality scores.\\nNoise Robustness appraises the model’s capability to man-\\nage noise documents that are question-related but lack sub-\\nstantive information.\\nNegative Rejection assesses the model’s discernment in\\nrefraining from responding when the retrieved documents do'),\n",
       " Document(metadata={'page': 16, 'source': 'RAG_survey.pdf'}, page_content='Y . Zhang, Y . Chenet al., “Siren’s song in the ai ocean: A survey on hal-\\nlucination in large language models,” arXiv preprint arXiv:2309.01219,\\n2023.\\n[3] D. Arora, A. Kini, S. R. Chowdhury, N. Natarajan, G. Sinha, and\\nA. Sharma, “Gar-meets-rag paradigm for zero-shot information re-\\ntrieval,” arXiv preprint arXiv:2310.20158 , 2023.\\n[4] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal,\\nH. K ¨uttler, M. Lewis, W.-t. Yih, T. Rockt ¨aschel et al. , “Retrieval-'),\n",
       " Document(metadata={'page': 14, 'source': 'RAG_survey.pdf'}, page_content='Knowledge-intensive QA\\nError Correction\\nSummarization\\nBLEU\\nROUGE-L\\nBertScore\\nRAGQuestEval\\n† represents a benchmark, and ‡ represents a tool. * denotes customized quantitative metrics, which deviate from traditional\\nmetrics. Readers are encouraged to consult pertinent literature for the specific quantification formulas associated with these\\nmetrics, as required.\\nand non-parameterized advantages are areas ripe for explo-\\nration [27]. Another trend is to introduce SLMs with specific'),\n",
       " Document(metadata={'page': 0, 'source': 'RAG_survey.pdf'}, page_content='RAG-Survey\\nin Figure 1. The development trajectory of RAG in the era\\nof large models exhibits several distinct stage characteristics.\\nInitially, RAG’s inception coincided with the rise of the\\nTransformer architecture, focusing on enhancing language\\nmodels by incorporating additional knowledge through Pre-\\nTraining Models (PTM). This early stage was characterized\\nby foundational work aimed at refining pre-training techniques\\n[3]–[5].The subsequent arrival of ChatGPT [6] marked a'),\n",
       " Document(metadata={'page': 1, 'source': 'RAG_survey.pdf'}, page_content='analyzes the three augmentation processes. Section VI focuses\\non RAG’s downstream tasks and evaluation system. Sec-\\ntion VII mainly discusses the challenges that RAG currently\\nfaces and its future development directions. At last, the paper\\nconcludes in Section VIII.\\nII. O VERVIEW OF RAG\\nA typical application of RAG is illustrated in Figure 2.\\nHere, a user poses a question to ChatGPT about a recent,\\nwidely discussed news. Given ChatGPT’s reliance on pre-'),\n",
       " Document(metadata={'page': 9, 'source': 'RAG_survey.pdf'}, page_content='in information retrieval, acting as both an enhancer and a\\nfilter, delivering refined inputs for more precise language\\nmodel processing [70]. Reranking can be performed using\\nrule-based methods that depend on predefined metrics like\\nDiversity, Relevance, and MRR, or model-based approaches\\nlike Encoder-Decoder models from the BERT series (e.g.,\\nSpanBERT), specialized reranking models such as Cohere\\nrerank or bge-raranker-large, and general large language mod-\\nels like GPT [12], [99].'),\n",
       " Document(metadata={'page': 18, 'source': 'RAG_survey.pdf'}, page_content='arXiv preprint arXiv:2011.01060 , 2020.\\n[120] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, “Musique:\\nMultihop questions via single-hop question composition,” Transactions\\nof the Association for Computational Linguistics , vol. 10, pp. 539–554,\\n2022.\\n[121] A. Fan, Y . Jernite, E. Perez, D. Grangier, J. Weston, and M. Auli, “Eli5:\\nLong form question answering,” arXiv preprint arXiv:1907.09190 ,\\n2019.\\n[122] T. Ko ˇcisk`y, J. Schwarz, P. Blunsom, C. Dyer, K. M. Hermann, G. Melis,'),\n",
       " Document(metadata={'page': 11, 'source': 'RAG_survey.pdf'}, page_content='decides when to activate retrieval, or alternatively, a predefined\\nthreshold may trigger the process. During retrieval, the gen-\\nerator conducts a fragment-level beam search across multiple\\nparagraphs to derive the most coherent sequence. Critic scores\\nare used to update the subdivision scores, with the flexibility\\nto adjust these weights during inference, tailoring the model’s\\nbehavior. Self-RAG’s design obviates the need for additional'),\n",
       " Document(metadata={'page': 19, 'source': 'RAG_survey.pdf'}, page_content='2018.\\n[146] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and\\nJ. Steinhardt, “Measuring massive multitask language understanding,”\\narXiv preprint arXiv:2009.03300 , 2020.\\n[147] S. Merity, C. Xiong, J. Bradbury, and R. Socher, “Pointer sentinel\\nmixture models,” arXiv preprint arXiv:1609.07843 , 2016.\\n[148] M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, and J. Berant,\\n“Did aristotle use a laptop? a question answering benchmark with'),\n",
       " Document(metadata={'page': 17, 'source': 'RAG_survey.pdf'}, page_content='irrelevant context,” in International Conference on Machine Learning .\\nPMLR, 2023, pp. 31 210–31 227.\\n[88] R. Teja, “Evaluating the ideal chunk size for a rag\\nsystem using llamaindex,” https://www.llamaindex.ai/blog/\\nevaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5,\\n2023.'),\n",
       " Document(metadata={'page': 2, 'source': 'RAG_survey.pdf'}, page_content='model is tasked with formulating a response. The model’s\\napproach to answering may vary depending on task-specific\\ncriteria, allowing it to either draw upon its inherent parametric\\nknowledge or restrict its responses to the information con-\\ntained within the provided documents. In cases of ongoing\\ndialogues, any existing conversational history can be integrated\\ninto the prompt, enabling the model to engage in multi-turn\\ndialogue interactions effectively.'),\n",
       " Document(metadata={'page': 18, 'source': 'RAG_survey.pdf'}, page_content='2019.\\n[122] T. Ko ˇcisk`y, J. Schwarz, P. Blunsom, C. Dyer, K. M. Hermann, G. Melis,\\nand E. Grefenstette, “The narrativeqa reading comprehension chal-\\nlenge,” Transactions of the Association for Computational Linguistics ,\\nvol. 6, pp. 317–328, 2018.\\n[123] K.-H. Lee, X. Chen, H. Furuta, J. Canny, and I. Fischer, “A human-\\ninspired reading agent with gist memory of very long contexts,” arXiv\\npreprint arXiv:2402.09727, 2024.\\n[124] I. Stelmakh, Y . Luan, B. Dhingra, and M.-W. Chang, “Asqa: Factoid'),\n",
       " Document(metadata={'page': 4, 'source': 'RAG_survey.pdf'}, page_content='(FT) and prompt engineering. Each method has distinct charac-\\nteristics as illustrated in Figure 4. We used a quadrant chart to\\nillustrate the differences among three methods in two dimen-\\nsions: external knowledge requirements and model adaption\\nrequirements. Prompt engineering leverages a model’s inherent\\ncapabilities with minimum necessity for external knowledge\\nand model adaption. RAG can be likened to providing a model\\nwith a tailored textbook for information retrieval, ideal for pre-'),\n",
       " Document(metadata={'page': 18, 'source': 'RAG_survey.pdf'}, page_content='[126] P. Dasigi, K. Lo, I. Beltagy, A. Cohan, N. A. Smith, and M. Gardner,\\n“A dataset of information-seeking questions and answers anchored in\\nresearch papers,” arXiv preprint arXiv:2105.03011 , 2021.\\n[127] T. M ¨oller, A. Reina, R. Jayakumar, and M. Pietsch, “Covid-qa: A\\nquestion answering dataset for covid-19,” in ACL 2020 Workshop on\\nNatural Language Processing for COVID-19 (NLP-COVID) , 2020.\\n[128] X. Wang, G. H. Chen, D. Song, Z. Zhang, Z. Chen, Q. Xiao, F. Jiang,'),\n",
       " Document(metadata={'page': 19, 'source': 'RAG_survey.pdf'}, page_content='“Did aristotle use a laptop? a question answering benchmark with\\nimplicit reasoning strategies,” Transactions of the Association for\\nComputational Linguistics, vol. 9, pp. 346–361, 2021.\\n[149] J. Thorne, A. Vlachos, C. Christodoulopoulos, and A. Mittal, “Fever: a\\nlarge-scale dataset for fact extraction and verification,” arXiv preprint\\narXiv:1803.05355, 2018.\\n[150] N. Kotonya and F. Toni, “Explainable automated fact-checking for\\npublic health claims,” arXiv preprint arXiv:2010.09926 , 2020.'),\n",
       " Document(metadata={'page': 13, 'source': 'RAG_survey.pdf'}, page_content='situation is figuratively referred to as “Misinformation can\\nbe worse than no information at all”. Improving RAG’s\\nresistance to such adversarial or counterfactual inputs is gain-\\ning research momentum and has become a key performance\\nmetric [48], [50], [82]. Cuconasu et al. [54] analyze which\\ntype of documents should be retrieved, evaluate the relevance\\nof the documents to the prompt, their position, and the\\nnumber included in the context. The research findings reveal'),\n",
       " Document(metadata={'page': 4, 'source': 'RAG_survey.pdf'}, page_content='A. Retrieval Source\\nRAG relies on external knowledge to enhance LLMs, while\\nthe type of retrieval source and the granularity of retrieval\\nunits both affect the final generation results.\\n1) Data Structure: Initially, text is s the mainstream source\\nof retrieval. Subsequently, the retrieval source expanded to in-\\nclude semi-structured data (PDF) and structured data (Knowl-\\nedge Graph, KG) for enhancement. In addition to retrieving\\nfrom original external sources, there is also a growing trend in'),\n",
       " Document(metadata={'page': 13, 'source': 'RAG_survey.pdf'}, page_content='14\\nTABLE III\\nSUMMARY OF METRICS APPLICABLE FOR EVALUATION ASPECTS OF RAG\\nContext\\nRelevance Faithfulness Answer\\nRelevance\\nNoise\\nRobustness\\nNegative\\nRejection\\nInformation\\nIntegration\\nCounterfactual\\nRobustness\\nAccuracy ✓ ✓ ✓ ✓ ✓ ✓ ✓\\nEM ✓\\nRecall ✓\\nPrecision ✓ ✓\\nR-Rate ✓\\nCosine Similarity ✓\\nHit Rate ✓\\nMRR ✓\\nNDCG ✓\\nBLEU ✓ ✓ ✓\\nROUGE/ROUGE-L ✓ ✓ ✓\\nThe specific metrics for each evaluation aspect are sum-\\nmarized in Table III. It is essential to recognize that these')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Multi-Query RAG Chain: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Construct a Multi-Query RAG Chain\n",
    "multi_query_rag_chain = (\n",
    "    retrieval_chain |\n",
    "    (lambda docs: {\"context\": \"\\n\\n\".join([doc.page_content for doc in docs]), \"question\": question}) |\n",
    "    prompt |\n",
    "    llm |\n",
    "    output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Example Comparisons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard RAG Output:\n",
      "The key challenges discussed in the paper include overcoming the limitations of Naive RAG, managing increasing context sizes that surpass the capabilities of traditional LLMs, and addressing specific shortcomings in the retrieval-augmentation process. These challenges are a response to the limitations of Naive RAG and warrant further research. The paper explores potential avenues for research and development to address these challenges.\n",
      "\n",
      "Multi-Query RAG Output:\n",
      "The paper discusses several key challenges related to Retrieval-Augmented Generation (RAG) models, including:\n",
      "\n",
      "1. Misinformation and counterfactual inputs: Improving RAG's resistance to adversarial or counterfactual inputs is a major challenge.\n",
      "2. Context relevance and noise robustness: Evaluating the quality of retrieval and generation requires assessing context relevance and noise robustness.\n",
      "3. External knowledge requirements: The type of retrieval source and granularity of retrieval units affect the final generation results.\n",
      "4. Semantic discontinuity and accumulation of irrelevant information: Retrieval iterations may be affected by semantic discontinuity and the accumulation of irrelevant information.\n",
      "5. Evaluation metrics: Developing effective evaluation metrics for RAG models is challenging, and existing metrics such as accuracy, recall, precision, and BLEU need to be adapted or combined to address these challenges.\n",
      "\n",
      "Overall, the paper highlights the complexity of evaluating RAG models and the need for further research and development in this area.\n"
     ]
    }
   ],
   "source": [
    "# Define the question\n",
    "question = \"What are the key challenges discussed in the paper?\"\n",
    "\n",
    "# Standard RAG output\n",
    "standard_rag_output = rag_chain.invoke(question)\n",
    "\n",
    "# Multi-query RAG output\n",
    "multi_query_rag_output = multi_query_rag_chain.invoke({\"question\": question})\n",
    "\n",
    "# Print the outputs\n",
    "print(\"Standard RAG Output:\")\n",
    "print(standard_rag_output)\n",
    "print(\"\\nMulti-Query RAG Output:\")\n",
    "print(multi_query_rag_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion:\n",
    "\n",
    "在這次實驗中，我們比較了標準 RAG 和多查詢 RAG 的輸出結果，以展示多查詢方法如何在處理複雜或模糊查詢時提供更全面或更細緻的答案。\n",
    "\n",
    "#### 標準 RAG 輸出：\n",
    "標準 RAG 的輸出主要集中在克服 Naive RAG 的局限性、管理超過傳統 LLM 能力的上下文大小以及解決檢索增強過程中的特定缺點。這些挑戰是對 Naive RAG 局限性的回應，並且需要進一步的研究。該論文探討了研究和開發的潛在途徑，以解決這些挑戰。\n",
    "\n",
    "#### 多查詢 RAG 輸出：\n",
    "多查詢 RAG 的輸出更為詳細，涵蓋了與檢索增強生成（RAG）模型相關的多個關鍵挑戰，包括：\n",
    "\n",
    "1. 錯誤信息和反事實輸入：提高 RAG 對抗對抗性或反事實輸入的抵抗力是一個主要挑戰。\n",
    "2. 上下文相關性和噪聲魯棒性：評估檢索和生成的質量需要評估上下文相關性和噪聲魯棒性。\n",
    "3. 外部知識需求：檢索源的類型和檢索單元的粒度會影響最終的生成結果。\n",
    "4. 語義不連續性和無關信息的積累：檢索迭代可能會受到語義不連續性和無關信息積累的影響。\n",
    "5. 評估指標：為 RAG 模型開發有效的評估指標具有挑戰性，現有的指標如準確性、召回率、精確度和 BLEU 需要進行調整或結合以應對這些挑戰。\n",
    "\n",
    "\n",
    "#### 比較與討論：\n",
    "從結果可以看出，多查詢 RAG 方法提供了更全面和細緻的答案。標準 RAG 方法雖然能夠識別出一些關鍵挑戰，但其輸出較為簡略，缺乏對具體問題的深入探討。相比之下，多查詢 RAG 方法能夠識別出更多的挑戰，並提供更詳細的解釋，這使得其在處理複雜或模糊查詢時更具優勢。\n",
    "\n",
    "多查詢方法的優勢在於它能夠從多個角度檢索和生成答案，從而提供更豐富的上下文信息和更全面的見解。這對於需要深入理解和分析的場景特別有用，例如研究報告、技術文檔或需要多方面考量的決策支持系統。\n",
    "\n",
    "總結來說，多查詢 RAG 方法在處理複雜或模糊查詢時能夠提供更全面和細緻的答案，這使得其在許多應用場景中具有顯著的優勢。未來的研究可以進一步探索如何優化多查詢方法，以提高其效率和準確性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part3 RAG Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are an expert in various fields. Please provide answers to the following question from different perspectives:\n",
    "1. As a scientist\n",
    "2. As a historian\n",
    "3. As a philosopher\n",
    "4. As an economist\n",
    "\n",
    "Original question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    "    | (lambda x : list(filter(lambda x : x != '', x)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I\\'d be happy to provide answers to the question \"What are the key challenges discussed in the paper?\" from different perspectives:',\n",
       " '**As a Scientist:**',\n",
       " 'The key challenges discussed in the paper appear to be related to the impact of climate change on human health and well-being. From a scientific perspective, the paper likely highlights the following challenges:',\n",
       " '1. Rising temperatures and extreme weather events: The paper may discuss the increasing frequency and severity of heatwaves, droughts, and storms, which can have devastating effects on human health.',\n",
       " '2. Air and water pollution: The paper may also highlight the challenges posed by air and water pollution, particularly in urban areas where high population densities and industrial activities contribute to environmental degradation.',\n",
       " '3. Vector-borne diseases: The paper likely addresses the risks of vector-borne diseases such as malaria, dengue fever, and Zika virus, which are exacerbated by climate change.',\n",
       " '4. Mental health: The paper may discuss the psychological impacts of living in a world with increasing climate-related stressors, including anxiety, depression, and post-traumatic stress disorder (PTSD).',\n",
       " 'Overall, the scientific perspective on the challenges discussed in the paper would focus on the empirical evidence and data-driven approaches to understanding the effects of climate change on human health.',\n",
       " '**As a Historian:**',\n",
       " \"From a historical perspective, the key challenges discussed in the paper may be seen as part of a broader narrative about humanity's relationship with the environment. The paper likely highlights the following challenges:\",\n",
       " '1. Historical context of environmental degradation: The paper may explore how past environmental degradation and climate change have had lasting impacts on human societies and ecosystems.',\n",
       " '2. Social justice and inequality: The paper likely addresses how historical climate-related disasters and environmental injustices have disproportionately affected marginalized communities and vulnerable populations.',\n",
       " '3. Cultural responses to environmental challenges: The paper may examine how different cultures and civilizations have responded to environmental challenges throughout history, including traditional practices and technologies for mitigating climate change impacts.',\n",
       " '4. Long-term thinking vs. short-term gains: The paper may discuss the tension between long-term thinking about climate change mitigation and the often shorter-term focus on economic growth or immediate benefits.',\n",
       " 'Overall, the historical perspective on the challenges discussed in the paper would consider the broader social, cultural, and environmental context of past climate-related events and their ongoing impacts.',\n",
       " '**As a Philosopher:**',\n",
       " 'From a philosophical perspective, the key challenges discussed in the paper may be seen as part of a deeper exploration of human values and ethics. The paper likely highlights the following challenges:',\n",
       " '1. Moral obligations to future generations: The paper may discuss the moral implications of climate change on our responsibilities to future generations and the planet.',\n",
       " '2. Value conflicts between economic growth and environmental protection: The paper likely explores the tension between pursuing economic growth and prioritizing environmental protection, considering questions like \"what is the value of a healthy environment?\"',\n",
       " '3. Epistemic uncertainty and risk perception: The paper may examine how our understanding of climate change impacts is shaped by epistemic uncertainty and individual perceptions of risk.',\n",
       " '4. Distributive justice and climate policy: The paper may address issues of distributive justice, examining how climate policies and decisions are made in a way that respects the needs and values of different populations.',\n",
       " 'Overall, the philosophical perspective on the challenges discussed in the paper would focus on exploring fundamental questions about human values, ethics, and morality in relation to climate change.',\n",
       " '**As an Economist:**',\n",
       " 'From an economic perspective, the key challenges discussed in the paper likely revolve around the financial implications of climate change. The paper may highlight the following challenges:',\n",
       " '1. Economic costs of climate change: The paper may estimate the economic costs of climate change-related disasters and mitigation efforts.',\n",
       " '2. Transition costs and policy design: The paper likely discusses how to effectively transition away from fossil fuels and design policies that minimize negative economic impacts on workers, businesses, and communities.',\n",
       " '3. Risk management and insurance: The paper may examine how climate-related risks can be managed through insurance mechanisms and other risk transfer strategies.',\n",
       " '4. Long-term thinking in economic policy: The paper may argue for the importance of incorporating long-term thinking into economic decision-making to accommodate the needs of future generations.',\n",
       " 'Overall, the economic perspective on the challenges discussed in the paper would focus on understanding the financial dimensions of climate change and developing effective policies to mitigate its impacts.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_queries.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Implement Reciprocal Rank Fusion (RRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(results: list[list], c=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            # Calculate the RRF score\n",
    "            score = 1 / (c + rank + 1)\n",
    "            # Update the fused score for the document\n",
    "            if doc_str in fused_scores:\n",
    "                fused_scores[doc_str] += score\n",
    "            else:\n",
    "                fused_scores[doc_str] = score\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) RRF Example and k-Value Discussion: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-ranked Documents using RRF:\n",
      "Score: 0.20744532492450257\n",
      "question, form a comprehensive prompt that empowers LLMs\n",
      "to generate a well-informed answer.\n",
      "The RAG research paradigm is continuously evolving, and\n",
      "we categorize it into three stages: Naive RAG, Advanced\n",
      "RAG, and Modular RAG, as showed in Figure 3. Despite\n",
      "RAG method are cost-effective and surpass the performance\n",
      "of the native LLM, they also exhibit several limitations.\n",
      "The development of Advanced RAG and Modular RAG is\n",
      "a response to these specific shortcomings in Naive RAG.\n",
      "A. Naive RAG\n",
      "\n",
      "Score: 0.15954739136091595\n",
      "(FT) and prompt engineering. Each method has distinct charac-\n",
      "teristics as illustrated in Figure 4. We used a quadrant chart to\n",
      "illustrate the differences among three methods in two dimen-\n",
      "sions: external knowledge requirements and model adaption\n",
      "requirements. Prompt engineering leverages a model’s inherent\n",
      "capabilities with minimum necessity for external knowledge\n",
      "and model adaption. RAG can be likened to providing a model\n",
      "with a tailored textbook for information retrieval, ideal for pre-\n",
      "\n",
      "Score: 0.1446904405160619\n",
      "model is tasked with formulating a response. The model’s\n",
      "approach to answering may vary depending on task-specific\n",
      "criteria, allowing it to either draw upon its inherent parametric\n",
      "knowledge or restrict its responses to the information con-\n",
      "tained within the provided documents. In cases of ongoing\n",
      "dialogues, any existing conversational history can be integrated\n",
      "into the prompt, enabling the model to engage in multi-turn\n",
      "dialogue interactions effectively.\n",
      "\n",
      "Score: 0.11192758101966194\n",
      "eral challenges persist that warrant in-depth research.This\n",
      "chapter will mainly introduce the current challenges and future\n",
      "research directions faced by RAG.\n",
      "A. RAG vs Long Context\n",
      "With the deepening of related research, the context of LLMs\n",
      "is continuously expanding [170]–[172]. Presently, LLMs can\n",
      "effortlessly manage contexts exceeding 200,000 tokens 9. This\n",
      "capability signifies that long-document question answering,\n",
      "previously reliant on RAG, can now incorporate the entire\n",
      "\n",
      "Score: 0.11017433690787737\n",
      "may face the issue of hallucination, where it produces con-\n",
      "tent not supported by the retrieved context. This phase can\n",
      "also suffer from irrelevance, toxicity, or bias in the outputs,\n",
      "detracting from the quality and reliability of the responses.\n",
      "Augmentation Hurdles . Integrating retrieved information\n",
      "with the different task can be challenging, sometimes resulting\n",
      "in disjointed or incoherent outputs. The process may also\n",
      "encounter redundancy when similar information is retrieved\n",
      "\n",
      "Score: 0.0970633923849815\n",
      "Answer Faithfulness ensures that the generated answers\n",
      "remain true to the retrieved context, maintaining consistency\n",
      "and avoiding contradictions.\n",
      "Answer Relevance requires that the generated answers are\n",
      "directly pertinent to the posed questions, effectively addressing\n",
      "the core inquiry.\n",
      "2) Required Abilities: RAG evaluation also encompasses\n",
      "four abilities indicative of its adaptability and efficiency:\n",
      "noise robustness, negative rejection, information integration,\n",
      "\n",
      "Score: 0.07887704813108039\n",
      "“Did aristotle use a laptop? a question answering benchmark with\n",
      "implicit reasoning strategies,” Transactions of the Association for\n",
      "Computational Linguistics, vol. 9, pp. 346–361, 2021.\n",
      "[149] J. Thorne, A. Vlachos, C. Christodoulopoulos, and A. Mittal, “Fever: a\n",
      "large-scale dataset for fact extraction and verification,” arXiv preprint\n",
      "arXiv:1803.05355, 2018.\n",
      "[150] N. Kotonya and F. Toni, “Explainable automated fact-checking for\n",
      "public health claims,” arXiv preprint arXiv:2010.09926 , 2020.\n",
      "\n",
      "Score: 0.06430053288858154\n",
      "situation is figuratively referred to as “Misinformation can\n",
      "be worse than no information at all”. Improving RAG’s\n",
      "resistance to such adversarial or counterfactual inputs is gain-\n",
      "ing research momentum and has become a key performance\n",
      "metric [48], [50], [82]. Cuconasu et al. [54] analyze which\n",
      "type of documents should be retrieved, evaluate the relevance\n",
      "of the documents to the prompt, their position, and the\n",
      "number included in the context. The research findings reveal\n",
      "\n",
      "Score: 0.0640445165035329\n",
      "2\n",
      "Fig. 1. Technology tree of RAG research. The stages of involving RAG mainly include pre-training, fine-tuning, and inference. With the emergence of LLMs,\n",
      "research on RAG initially focused on leveraging the powerful in context learning abilities of LLMs, primarily concentrating on the inference stage. Subsequent\n",
      "research has delved deeper, gradually integrating more with the fine-tuning of LLMs. Researchers have also been exploring ways to enhance language models\n",
      "\n",
      "Score: 0.06201923076923077\n",
      "14\n",
      "TABLE III\n",
      "SUMMARY OF METRICS APPLICABLE FOR EVALUATION ASPECTS OF RAG\n",
      "Context\n",
      "Relevance Faithfulness Answer\n",
      "Relevance\n",
      "Noise\n",
      "Robustness\n",
      "Negative\n",
      "Rejection\n",
      "Information\n",
      "Integration\n",
      "Counterfactual\n",
      "Robustness\n",
      "Accuracy ✓ ✓ ✓ ✓ ✓ ✓ ✓\n",
      "EM ✓\n",
      "Recall ✓\n",
      "Precision ✓ ✓\n",
      "R-Rate ✓\n",
      "Cosine Similarity ✓\n",
      "Hit Rate ✓\n",
      "MRR ✓\n",
      "NDCG ✓\n",
      "BLEU ✓ ✓ ✓\n",
      "ROUGE/ROUGE-L ✓ ✓ ✓\n",
      "The specific metrics for each evaluation aspect are sum-\n",
      "marized in Table III. It is essential to recognize that these\n",
      "\n",
      "Score: 0.047643442622950824\n",
      "1) Quality Scores: Quality scores include context rele-\n",
      "vance, answer faithfulness, and answer relevance. These qual-\n",
      "ity scores evaluate the efficiency of the RAG model from\n",
      "different perspectives in the process of information retrieval\n",
      "and generation [164]–[166].\n",
      "Context Relevance evaluates the precision and specificity\n",
      "of the retrieved context, ensuring relevance and minimizing\n",
      "processing costs associated with extraneous content.\n",
      "Answer Faithfulness ensures that the generated answers\n",
      "\n",
      "Score: 0.04738666351569577\n",
      "opportunities for the development of RAG, enabling it to\n",
      "address more complex problems and integrative or summary\n",
      "questions that require reading a large amount of material to\n",
      "answer [49]. Developing new RAG methods in the context of\n",
      "super-long contexts is one of the future research trends.\n",
      "B. RAG Robustness\n",
      "The presence of noise or contradictory information during\n",
      "retrieval can detrimentally affect RAG’s output quality. This\n",
      "situation is figuratively referred to as “Misinformation can\n",
      "\n",
      "Score: 0.04713064713064713\n",
      "2018.\n",
      "[146] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and\n",
      "J. Steinhardt, “Measuring massive multitask language understanding,”\n",
      "arXiv preprint arXiv:2009.03300 , 2020.\n",
      "[147] S. Merity, C. Xiong, J. Bradbury, and R. Socher, “Pointer sentinel\n",
      "mixture models,” arXiv preprint arXiv:1609.07843 , 2016.\n",
      "[148] M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, and J. Berant,\n",
      "“Did aristotle use a laptop? a question answering benchmark with\n",
      "\n",
      "Score: 0.04639423076923077\n",
      "A. Retrieval Source\n",
      "RAG relies on external knowledge to enhance LLMs, while\n",
      "the type of retrieval source and the granularity of retrieval\n",
      "units both affect the final generation results.\n",
      "1) Data Structure: Initially, text is s the mainstream source\n",
      "of retrieval. Subsequently, the retrieval source expanded to in-\n",
      "clude semi-structured data (PDF) and structured data (Knowl-\n",
      "edge Graph, KG) for enhancement. In addition to retrieving\n",
      "from original external sources, there is also a growing trend in\n",
      "\n",
      "Score: 0.032266458495966696\n",
      "irrelevant context,” in International Conference on Machine Learning .\n",
      "PMLR, 2023, pp. 31 210–31 227.\n",
      "[88] R. Teja, “Evaluating the ideal chunk size for a rag\n",
      "system using llamaindex,” https://www.llamaindex.ai/blog/\n",
      "evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5,\n",
      "2023.\n",
      "\n",
      "Score: 0.032266458495966696\n",
      "noise robustness, negative rejection, information integration,\n",
      "and counterfactual robustness [167], [168]. These abilities are\n",
      "critical for the model’s performance under various challenges\n",
      "and complex scenarios, impacting the quality scores.\n",
      "Noise Robustness appraises the model’s capability to man-\n",
      "age noise documents that are question-related but lack sub-\n",
      "stantive information.\n",
      "Negative Rejection assesses the model’s discernment in\n",
      "refraining from responding when the retrieved documents do\n",
      "\n",
      "Score: 0.032018442622950824\n",
      "The paper unfolds as follows: Section II introduces the\n",
      "main concept and current paradigms of RAG. The following\n",
      "three sections explore core components—“Retrieval”, “Gen-\n",
      "eration” and “Augmentation”, respectively. Section III focuses\n",
      "on optimization methods in retrieval,including indexing, query\n",
      "and embedding optimization. Section IV concentrates on post-\n",
      "retrieval process and LLM fine-tuning in generation. Section V\n",
      "analyzes the three augmentation processes. Section VI focuses\n",
      "\n",
      "Score: 0.03200204813108039\n",
      "B. Evaluation Target\n",
      "Historically, RAG models assessments have centered on\n",
      "their execution in specific downstream tasks. These evaluations\n",
      "employ established metrics suitable to the tasks at hand. For\n",
      "instance, question answering evaluations might rely on EM\n",
      "and F1 scores [7], [45], [59], [72], whereas fact-checking\n",
      "tasks often hinge on Accuracy as the primary metric [4],\n",
      "[14], [42]. BLEU and ROUGE metrics are also commonly\n",
      "used to evaluate answer quality [26], [32], [52], [78]. Tools\n",
      "\n",
      "Score: 0.03177805800756621\n",
      "the challenges currently faced and points out prospective avenues\n",
      "for research and development 1.\n",
      "Index Terms—Large language model, retrieval-augmented gen-\n",
      "eration, natural language processing, information retrieval\n",
      "I. I NTRODUCTION\n",
      "L\n",
      "ARGE language models (LLMs) have achieved remark-\n",
      "able success, though they still face significant limitations,\n",
      "especially in domain-specific or knowledge-intensive tasks [1],\n",
      "notably producing “hallucinations” [2] when handling queries\n",
      "\n",
      "Score: 0.031754032258064516\n",
      "7\n",
      "Fig. 4. RAG compared with other model optimization methods in the aspects of “External Knowledge Required” and “Model Adaption Required”. Prompt\n",
      "Engineering requires low modifications to the model and external knowledge, focusing on harnessing the capabilities of LLMs themselves. Fine-tuning, on\n",
      "the other hand, involves further training the model. In the early stages of RAG (Naive RAG), there is a low demand for model modifications. As research\n",
      "\n",
      "Score: 0.031746031746031744\n",
      "a response to these specific shortcomings in Naive RAG.\n",
      "A. Naive RAG\n",
      "The Naive RAG research paradigm represents the earli-\n",
      "est methodology, which gained prominence shortly after the\n",
      "\n",
      "Score: 0.0315136476426799\n",
      "search,” arXiv preprint arXiv:2311.03758 , 2023.\n",
      "[10] H. S. Zheng, S. Mishra, X. Chen, H.-T. Cheng, E. H. Chi, Q. V . Le,\n",
      "and D. Zhou, “Take a step back: Evoking reasoning via abstraction in\n",
      "large language models,” arXiv preprint arXiv:2310.06117 , 2023.\n",
      "[11] L. Gao, X. Ma, J. Lin, and J. Callan, “Precise zero-shot dense retrieval\n",
      "without relevance labels,” arXiv preprint arXiv:2212.10496 , 2022.\n",
      "[12] V . Blagojevi, “Enhancing rag pipelines in haystack: Introducing diver-\n",
      "\n",
      "Score: 0.03125763125763126\n",
      "A question answering challenge targeting commonsense knowledge,”\n",
      "arXiv preprint arXiv:1811.00937 , 2018.\n",
      "[133] E. Dinan, S. Roller, K. Shuster, A. Fan, M. Auli, and J. Weston,\n",
      "“Wizard of wikipedia: Knowledge-powered conversational agents,”\n",
      "arXiv preprint arXiv:1811.01241 , 2018.\n",
      "[134] H. Wang, M. Hu, Y . Deng, R. Wang, F. Mi, W. Wang, Y . Wang, W.-\n",
      "C. Kwan, I. King, and K.-F. Wong, “Large language models as source\n",
      "\n",
      "Score: 0.031009615384615385\n",
      "in information retrieval, acting as both an enhancer and a\n",
      "filter, delivering refined inputs for more precise language\n",
      "model processing [70]. Reranking can be performed using\n",
      "rule-based methods that depend on predefined metrics like\n",
      "Diversity, Relevance, and MRR, or model-based approaches\n",
      "like Encoder-Decoder models from the BERT series (e.g.,\n",
      "SpanBERT), specialized reranking models such as Cohere\n",
      "rerank or bge-raranker-large, and general large language mod-\n",
      "els like GPT [12], [99].\n",
      "\n",
      "Score: 0.031009615384615385\n",
      "conference on empirical methods in natural language processing, 2013,\n",
      "pp. 1533–1544.\n",
      "[116] A. Mallen, A. Asai, V . Zhong, R. Das, H. Hajishirzi, and D. Khashabi,\n",
      "“When not to trust language models: Investigating effectiveness and\n",
      "limitations of parametric and non-parametric memories,” arXiv preprint\n",
      "arXiv:2212.10511, 2022.\n",
      "[117] T. Nguyen, M. Rosenberg, X. Song, J. Gao, S. Tiwary, R. Majumder,\n",
      "and L. Deng, “Ms marco: A human-generated machine reading com-\n",
      "prehension dataset,” 2016.\n",
      "\n",
      "Score: 0.01639344262295082\n",
      "encounter redundancy when similar information is retrieved\n",
      "from multiple sources, leading to repetitive responses. Deter-\n",
      "mining the significance and relevance of various passages and\n",
      "ensuring stylistic and tonal consistency add further complexity.\n",
      "Facing complex issues, a single retrieval based on the original\n",
      "query may not suffice to acquire adequate context information.\n",
      "Moreover, there’s a concern that generation models might\n",
      "overly rely on augmented information, leading to outputs that\n",
      "\n",
      "Score: 0.01639344262295082\n",
      "NarrativeQA(NQA) [122] [45], [60], [63], [123]\n",
      "ASQA [124] [24], [57]\n",
      "QMSum(QM) [125] [60], [123]\n",
      "Domain QA Qasper [126] [60], [63]\n",
      "COVID-QA [127] [35], [46]\n",
      "CMB [128],MMCU Medical [129] [81]\n",
      "Multi-Choice QA QuALITY [130] [60], [63]\n",
      "ARC [131] [25], [67]\n",
      "CommonsenseQA [132] [58], [66]\n",
      "Graph QA GraphQA [84] [84]\n",
      "Dialog Dialog Generation Wizard of Wikipedia (WoW) [133] [13], [27], [34], [42]\n",
      "Personal Dialog KBP [134] [74], [135]\n",
      "DuleMon [136] [74]\n",
      "Task-oriented Dialog CamRest [137] [78], [79]\n",
      "\n",
      "Score: 0.016129032258064516\n",
      "[128] X. Wang, G. H. Chen, D. Song, Z. Zhang, Z. Chen, Q. Xiao, F. Jiang,\n",
      "J. Li, X. Wan, B. Wang et al. , “Cmb: A comprehensive medical\n",
      "benchmark in chinese,” arXiv preprint arXiv:2308.08833 , 2023.\n",
      "[129] H. Zeng, “Measuring massive multitask chinese understanding,” arXiv\n",
      "preprint arXiv:2304.12986, 2023.\n",
      "[130] R. Y . Pang, A. Parrish, N. Joshi, N. Nangia, J. Phang, A. Chen, V . Pad-\n",
      "makumar, J. Ma, J. Thompson, H. He et al. , “Quality: Question an-\n",
      "\n",
      "Score: 0.016129032258064516\n",
      "foundational technology stack, laying the groundwork for\n",
      "advanced enterprise applications. However, a fully integrated,\n",
      "comprehensive platform concept is still in the future, requiring\n",
      "further innovation and development.\n",
      "F . Multi-modal RAG\n",
      "RAG has transcended its initial text-based question-\n",
      "answering confines, embracing a diverse array of modal data.\n",
      "This expansion has spawned innovative multimodal models\n",
      "that integrate RAG concepts across various domains:\n",
      "\n",
      "Score: 0.016129032258064516\n",
      "decides when to activate retrieval, or alternatively, a predefined\n",
      "threshold may trigger the process. During retrieval, the gen-\n",
      "erator conducts a fragment-level beam search across multiple\n",
      "paragraphs to derive the most coherent sequence. Critic scores\n",
      "are used to update the subdivision scores, with the flexibility\n",
      "to adjust these weights during inference, tailoring the model’s\n",
      "behavior. Self-RAG’s design obviates the need for additional\n",
      "\n",
      "Score: 0.016129032258064516\n",
      "2019.\n",
      "[122] T. Ko ˇcisk`y, J. Schwarz, P. Blunsom, C. Dyer, K. M. Hermann, G. Melis,\n",
      "and E. Grefenstette, “The narrativeqa reading comprehension chal-\n",
      "lenge,” Transactions of the Association for Computational Linguistics ,\n",
      "vol. 6, pp. 317–328, 2018.\n",
      "[123] K.-H. Lee, X. Chen, H. Furuta, J. Canny, and I. Fischer, “A human-\n",
      "inspired reading agent with gist memory of very long contexts,” arXiv\n",
      "preprint arXiv:2402.09727, 2024.\n",
      "[124] I. Stelmakh, Y . Luan, B. Dhingra, and M.-W. Chang, “Asqa: Factoid\n",
      "\n",
      "Score: 0.016129032258064516\n",
      "Knowledge-intensive QA\n",
      "Error Correction\n",
      "Summarization\n",
      "BLEU\n",
      "ROUGE-L\n",
      "BertScore\n",
      "RAGQuestEval\n",
      "† represents a benchmark, and ‡ represents a tool. * denotes customized quantitative metrics, which deviate from traditional\n",
      "metrics. Readers are encouraged to consult pertinent literature for the specific quantification formulas associated with these\n",
      "metrics, as required.\n",
      "and non-parameterized advantages are areas ripe for explo-\n",
      "ration [27]. Another trend is to introduce SLMs with specific\n",
      "\n",
      "Score: 0.016129032258064516\n",
      "or reinforcement learning) [26]. For example, this can involve\n",
      "fine-tuning the retriever for better retrieval results, fine-tuning\n",
      "the generator for more personalized outputs, or engaging in\n",
      "collaborative fine-tuning [27].\n",
      "D. RAG vs Fine-tuning\n",
      "The augmentation of LLMs has attracted considerable atten-\n",
      "tion due to their growing prevalence. Among the optimization\n",
      "methods for LLMs, RAG is often compared with Fine-tuning\n",
      "(FT) and prompt engineering. Each method has distinct charac-\n",
      "\n",
      "Score: 0.015873015873015872\n",
      "centers on the generator’s capacity to synthesize coherent and\n",
      "relevant answers from the retrieved context. This evaluation\n",
      "can be categorized based on the content’s objectives: unlabeled\n",
      "and labeled content. For unlabeled content, the evaluation\n",
      "encompasses the faithfulness, relevance, and non-harmfulness\n",
      "of the generated answers. In contrast, for labeled content,\n",
      "the focus is on the accuracy of the information produced by\n",
      "the model [161]. Additionally, both retrieval and generation\n",
      "\n",
      "Score: 0.015873015873015872\n",
      "performance. The optimization process involving RAG and FT\n",
      "may require multiple iterations to achieve satisfactory results.\n",
      "III. R ETRIEVAL\n",
      "In the context of RAG, it is crucial to efficiently retrieve\n",
      "relevant documents from the data source. There are several\n",
      "key issues involved, such as the retrieval source, retrieval\n",
      "granularity, pre-processing of the retrieval, and selection of\n",
      "the corresponding embedding model.\n",
      "A. Retrieval Source\n",
      "RAG relies on external knowledge to enhance LLMs, while\n",
      "\n",
      "Score: 0.015873015873015872\n",
      "4\n",
      "Fig. 3. Comparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle)\n",
      "Advanced RAG proposes multiple optimization strategies around pre-retrieval and post-retrieval, with a process similar to the Naive RAG, still following a\n",
      "chain-like structure. (Right) Modular RAG inherits and develops from the previous paradigm, showcasing greater flexibility overall. This is evident in the\n",
      "\n",
      "Score: 0.015873015873015872\n",
      "In addition to aligning with human preferences, it is also\n",
      "possible to align with the preferences of fine-tuned models\n",
      "and retrievers [79]. When circumstances prevent access to\n",
      "powerful proprietary models or larger parameter open-source\n",
      "models, a simple and effective method is to distill the more\n",
      "powerful models(e.g. GPT-4). Fine-tuning of LLM can also\n",
      "be coordinated with fine-tuning of the retriever to align pref-\n",
      "erences. A typical approach, such as RA-DIT [27], aligns the\n",
      "\n",
      "Score: 0.015625\n",
      "question during retrieval to reduce the semantic gap between\n",
      "the question and the answer.\n",
      "3) Structural Index: One effective method for enhancing\n",
      "information retrieval is to establish a hierarchical structure for\n",
      "the documents. By constructing In structure, RAG system can\n",
      "expedite the retrieval and processing of pertinent data.\n",
      "Hierarchical index structure . File are arranged in parent-\n",
      "child relationships, with chunks linked to them. Data sum-\n",
      "\n",
      "Score: 0.015625\n",
      "refraining from responding when the retrieved documents do\n",
      "not contain the necessary knowledge to answer a question.\n",
      "Information Integration evaluates the model’s proficiency in\n",
      "synthesizing information from multiple documents to address\n",
      "complex questions.\n",
      "Counterfactual Robustness tests the model’s ability to rec-\n",
      "ognize and disregard known inaccuracies within documents,\n",
      "even when instructed about potential misinformation.\n",
      "Context relevance and noise robustness are important for\n",
      "\n",
      "Score: 0.015625\n",
      "consistency. It delineates the connections between different\n",
      "concepts and entities, markedly reducing the potential for\n",
      "illusions. Another advantage is the transformation of the\n",
      "information retrieval process into instructions that LLM can\n",
      "comprehend, thereby enhancing the accuracy of knowledge\n",
      "retrieval and enabling LLM to generate contextually coherent\n",
      "responses, thus improving the overall efficiency of the RAG\n",
      "system. To capture the logical relationship between document\n",
      "\n",
      "Score: 0.015625\n",
      "makumar, J. Ma, J. Thompson, H. He et al. , “Quality: Question an-\n",
      "swering with long input texts, yes!” arXiv preprint arXiv:2112.08608 ,\n",
      "2021.\n",
      "[131] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,\n",
      "and O. Tafjord, “Think you have solved question answering? try arc,\n",
      "the ai2 reasoning challenge,” arXiv preprint arXiv:1803.05457 , 2018.\n",
      "[132] A. Talmor, J. Herzig, N. Lourie, and J. Berant, “Commonsenseqa:\n",
      "A question answering challenge targeting commonsense knowledge,”\n",
      "\n",
      "Score: 0.015384615384615385\n",
      "focuses on embedding similarity from answer to answer rather\n",
      "than seeking embedding similarity for the problem or query.\n",
      "Using the Step-back Prompting method [10], the original\n",
      "query is abstracted to generate a high-level concept question\n",
      "(step-back question). In the RAG system, both the step-back\n",
      "question and the original query are used for retrieval, and both\n",
      "the results are utilized as the basis for language model answer\n",
      "generation.\n",
      "3) Query Routing: Based on varying queries, routing to\n",
      "\n",
      "Score: 0.015384615384615385\n",
      "arXiv:2308.11761, 2023.\n",
      "[16] A. H. Raudaschl, “Forget rag, the future\n",
      "is rag-fusion,” https://towardsdatascience.com/\n",
      "forget-rag-the-future-is-rag-fusion-1147298d8ad1, 2023.\n",
      "[17] X. Cheng, D. Luo, X. Chen, L. Liu, D. Zhao, and R. Yan, “Lift\n",
      "yourself up: Retrieval-augmented text generation with self memory,”\n",
      "arXiv preprint arXiv:2305.02437 , 2023.\n",
      "[18] S. Wang, Y . Xu, Y . Fang, Y . Liu, S. Sun, R. Xu, C. Zhu, and\n",
      "M. Zeng, “Training data is more valuable than you think: A simple\n",
      "\n",
      "Score: 0.015384615384615385\n",
      "within the RAG framework: Naive, Advanced, and Modu-\n",
      "lar RAG, each representing a progressive enhancement over\n",
      "its predecessors. RAG’s technical integration with other AI\n",
      "methodologies, such as fine-tuning and reinforcement learning,\n",
      "has further expanded its capabilities. Despite the progress in\n",
      "RAG technology, there are research opportunities to improve\n",
      "its robustness and its ability to handle extended contexts.\n",
      "RAG’s application scope is expanding into multimodal do-\n",
      "\n",
      "Score: 0.015384615384615385\n",
      "retrieving pertinent knowledge, which in turn facilitates the\n",
      "generation of improved responses in subsequent iterations.\n",
      "B. Recursive Retrieval\n",
      "Recursive retrieval is often used in information retrieval and\n",
      "NLP to improve the depth and relevance of search results.\n",
      "The process involves iteratively refining search queries based\n",
      "on the results obtained from previous searches. Recursive\n",
      "Retrieval aims to enhance the search experience by gradu-\n",
      "\n",
      "Score: 0.015384615384615385\n",
      "Context Relevance\n",
      "Faithfulness\n",
      "Answer Relevance\n",
      "*\n",
      "*\n",
      "Cosine Similarity\n",
      "ARES‡ Retrieval Quality\n",
      "Generation Quality\n",
      "Context Relevance\n",
      "Faithfulness\n",
      "Answer Relevance\n",
      "Accuracy\n",
      "Accuracy\n",
      "Accuracy\n",
      "TruLens‡ Retrieval Quality\n",
      "Generation Quality\n",
      "Context Relevance\n",
      "Faithfulness\n",
      "Answer Relevance\n",
      "*\n",
      "*\n",
      "*\n",
      "CRUD† Retrieval Quality\n",
      "Generation Quality\n",
      "Creative Generation\n",
      "Knowledge-intensive QA\n",
      "Error Correction\n",
      "Summarization\n",
      "BLEU\n",
      "ROUGE-L\n",
      "BertScore\n",
      "RAGQuestEval\n",
      "\n",
      "Score: 0.015384615384615385\n",
      "its indexing techniques through the use of a sliding window\n",
      "approach, fine-grained segmentation, and the incorporation of\n",
      "metadata. Additionally, it incorporates several optimization\n",
      "methods to streamline the retrieval process [8].\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the question\n",
    "question = \"What are the key challenges discussed in the paper?\"\n",
    "\n",
    "# Retrieve and re-rank documents using RRF\n",
    "reranked_docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "\n",
    "# Print the re-ranked documents\n",
    "print(\"Re-ranked Documents using RRF:\")\n",
    "for doc, score in reranked_docs:\n",
    "    print(f\"Score: {score}\")\n",
    "    print(doc.page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion:\n",
    "\n",
    "在這次實驗中，我們展示了使用 RRF（Ranked Retrieval Fusion）重新排序文檔的例子，並討論了 RRF 中的 c 值如何影響結果，以及 RRF 在哪些情境下具有優勢。\n",
    "\n",
    "#### 重新排序的文檔使用 RRF：\n",
    "結果顯示，使用 RRF 重新排序後的文檔得分如下：\n",
    "\n",
    "1. **Score: 0.20744532492450257**\n",
    "   - 這篇文檔討論了 RAG 研究範式的演變，包括 Naive RAG、Advanced RAG 和 Modular RAG 的三個階段，並指出了 RAG 方法的成本效益和性能優勢，同時也提到了其局限性。\n",
    "\n",
    "2. **Score: 0.15954739136091595**\n",
    "   - 這篇文檔介紹了不同方法的特點，使用象限圖展示了三種方法在外部知識需求和模型適應需求兩個維度上的差異。\n",
    "\n",
    "3. **Score: 0.1446904405160619**\n",
    "   - 這篇文檔描述了模型在回答問題時的不同方法，並討論了在多輪對話中整合現有對話歷史的能力。\n",
    "\n",
    "4. **Score: 0.11192758101966194**\n",
    "   - 這篇文檔主要介紹了 RAG 面臨的挑戰和未來的研究方向，特別是 RAG 與長上下文的關係。\n",
    "\n",
    "5. **Score: 0.11017433690787737**\n",
    "   - 這篇文檔討論了幻覺問題、無關內容和偏見對 RAG 輸出的影響，以及增強過程中的挑戰。\n",
    "\n",
    "#### c 值在 RRF 中的影響：\n",
    "RRF 中的 c 值是一個調整參數，用於平衡不同檢索器的影響。較大的 c 值會增加高排名文檔的權重，使得這些文檔在最終排序中更具優勢。相反，較小的 c 值會減少高排名文檔的權重，使得更多的文檔有機會進入最終排序。\n",
    "\n",
    "在這次實驗中，我們可以看到，使用 RRF 重新排序後，得分較高的文檔更集中於與查詢高度相關的內容，這表明 RRF 能夠有效地提升檢索結果的相關性和質量。\n",
    "\n",
    "#### RRF 的適用場景：\n",
    "RRF 在以下情境中特別有用：\n",
    "\n",
    "1. **多檢索器融合**：\n",
    "   - 當使用多個檢索器進行檢索時，RRF 可以有效地融合不同檢索器的結果，從而提高最終排序的準確性和相關性。\n",
    "\n",
    "2. **處理複雜查詢**：\n",
    "   - 對於複雜或模糊的查詢，RRF 能夠通過重新排序，將最相關的文檔提升到前面，從而提供更準確和全面的答案。\n",
    "\n",
    "3. **提高檢索質量**：\n",
    "   - RRF 可以平衡不同檢索器的影響，避免單一檢索器的偏差，從而提高檢索結果的整體質量。\n",
    "\n",
    "總結來說，RRF 是一種強大的重新排序技術，能夠通過調整 c 值來平衡不同檢索器的影響，從而提高檢索結果的相關性和質量。在多檢索器融合和處理複雜查詢的場景中，RRF 特別有用，能夠提供更準確和全面的答案。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) RAG Fusion Chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Implement the RAG Fusion chain\n",
    "rag_fusion_chain = (\n",
    "    retrieval_chain_rag_fusion |\n",
    "    (lambda docs: {\"context\": \"\\n\\n\".join([doc.page_content for doc, _ in docs]), \"question\": question}) |\n",
    "    prompt |\n",
    "    llm |\n",
    "    output_parser\n",
    ")\n",
    "\n",
    "res = rag_fusion_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper discusses several key challenges related to Retrieval-Augmented Generation (RAG) models. These include:\n",
      "\n",
      "1. **Retrieval Quality**: Evaluating the effectiveness of the context sourced by RAG, which is crucial for determining its overall quality.\n",
      "2. **Hybrid Approaches**: Combining RAG with fine-tuning is becoming increasingly popular as a way to enhance model performance.\n",
      "3. **Optimization Process**: The optimization process involving RAG and fine-tuning may require multiple iterations to achieve satisfactory results.\n",
      "4. **Noise Robustness**: RAG models need to be able to handle noise and irrelevant information in the retrieved documents, which can impact their accuracy.\n",
      "5. **Context Relevance**: Ensuring that the model is relevantly informative and focused on answering specific questions.\n",
      "6. **Hybrid Approaches**: Combining RAG with fine-tuning is emerging as a leading approach for enhancing model performance.\n",
      "7. **Incorporating Irrelevant Documents**: Including irrelevant documents can unexpectedly increase accuracy, highlighting the need for further research into the robustness of RAG.\n",
      "\n",
      "These challenges highlight the importance of developing specialized strategies to integrate retrieval with language generation models, such as RAG.\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
